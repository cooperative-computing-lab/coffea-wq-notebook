{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HEP analysis using Coffea + Work Queue Executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatically reload python modules when they are modified\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Work Queue](https://cctools.readthedocs.io/en/stable/work_queue/) is a system and library for creating and managing scalable manager-worker style programs that scale up to thousands of machines on clusters, clouds, and grids. With Work Queue, Coffea sends the work to be done to a set of Work Queue workers. These workers may be running locally, remotely in a cluster, or in some container environment.\n",
    "\n",
    "<img src=\"images/coffea-wq-general.png\" title=\"Coffea Work Queue basic architecture\" style=\"display:block; margin:auto;\"/>\n",
    "\n",
    "When used together with Coffea, it can measure the resources, such as cores and memory, that chunks of events need and adapt the allocations automatically to maximize throughput. In this notebook we will demonstrate how the executor can dynamically modify the size of chunks of events to process if the memory available is not enough, and adapt it to desired resources usage. But as a starter, let's first introduce its basic use for small local runs.\n",
    "\n",
    "and how it can automatically\n",
    "export the needed python environments when working in a cluster with no\n",
    "previous setup.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load coffea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# all coffea applications are run by coffea.processor.Runner\n",
    "from coffea.processor import Runner\n",
    "from coffea.processor import WorkQueueExecutor\n",
    "\n",
    "# workers will be created using this Factory\n",
    "from work_queue import Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test processor, modified from coffea source to make memory usage more interesting\n",
    "#from MemNanoTestProcessor import MemNanoTestProcessor as MyProcessor\n",
    "\n",
    "# or you can use the test processor included in coffea:\n",
    "\n",
    "# Use a sample processor from coffea\n",
    "# and silence coffea.hist deprecation, for now\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
    "from coffea.processor.test_items import NanoTestProcessor as MyProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define example input file. \n",
    "\n",
    "filelist = {\n",
    "    \"ttHJet\": [\"http://www.crc.nd.edu/~kmohrman/files/root_files/for_ci/ttHJet_UL17_R1B14_NAOD-00000_10194.root\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic configure for the WorkQueue Executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# For the most basic Work Queue configuration, we need to set the port that\n",
    "# the workers will use to communicate with the Coffea application.\n",
    "# The default for Work Queue is 9123, so we will use that here.\n",
    "# We can set Work Queue to find a free port for use, and we will explore\n",
    "# that later in the notebook.\n",
    "port = 9123\n",
    "executor = WorkQueueExecutor(port=port)\n",
    "runner = Runner(executor)\n",
    "\n",
    "#executor = WorkQueueExecutor(port=port, status_display_interval=5, extra_input_files=[\"MemNanoTestProcessor.py\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running one function at a time\n",
    "\n",
    "In Coffea, the iterative executor executes locally one function at a time. We can emulate this by directing the factory that creates workers to only create one worker. The factory automatically creates workers as it sees work available up to `max_workers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a factory that looks for a running Coffea in the local machine at the\n",
    "# port we defined above.\n",
    "workers = Factory(\"local\", manager_host_port = f\"localhost:{port}\")\n",
    "\n",
    "# Tell the factory that it should not create more than one worker...\n",
    "workers.max_workers = 1\n",
    "\n",
    "# ...and that the worker should not use more than 10GB of memory. If we do not set\n",
    "# resource limits, the worker automatically configures to use all available\n",
    "# in the machine, so the worker will use, for example, all the cores and memory in the\n",
    "# local machine.\n",
    "workers.disk = 10000  # Each worker won't use more than 10GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we execute the Coffea application. By using the factory inside a \n",
    "# \"with\" statement, the factory shutdowns the workers when the workflow finishes.\n",
    "\n",
    "with workers:\n",
    "    hists = runner(filelist, \"Events\", MyProcessor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can plot the results as usual\n",
    "%matplotlib inline\n",
    "import coffea.hist\n",
    "from IPython.utils import io\n",
    "coffea.hist.plot1d(hists[\"mass\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually declaring resources\n",
    "\n",
    "As we saw above, each task (that is, each function) used at most 1 core, and only a small portion of the memory and disk available. Thus, if our worker has more than 1 core, it could run more than one task concurrently. This can be achieved by explicitly specifying how many resources a task should use. Note how Work Queue automatically allocates to the tasks a corresponding proportion for the resources that we did not specify, (i.e. memory and disk):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executor = WorkQueueExecutor(port=port, cores=1)\n",
    "runner = Runner(executor)\n",
    "with workers:\n",
    "    hists = runner(filelist, \"Events\", MyProcessor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic resource management\n",
    "\n",
    "The previous example worked because we knew that each task did not use more than one core. Since we do not always know how many resources a function will need, the Work Queue executor automatically adjust its allocations according to previous values measured. Preprocessing, processing, and accumulating tasks are handled separately\n",
    "\n",
    "In fact, this is the default mode for the executor, only that we did not observe this in the previous examples because given the default Coffea chunksize (100k events), there were not enough tasks for Work Queue to figure out an allocation. In the absence of data, Work Queue allocates a whole worker to a task. Thus, for the automatic resource management example, we set the chunksize to a small value to create tons of tasks. Later in this notebook we will show how Work Queue can also adapt the chunksize to better use the resource allocations. \n",
    "\n",
    "In the output below, note how the `current allocation` of processing tasks eventually adapts to 1 core, even though there is no mention of cores when declaring the executor.\n",
    "\n",
    "Not that the automatic resource management depends on measuring the resources used by Coffea tasks, and this only works on Linux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executor = WorkQueueExecutor(port=port)\n",
    "runner = Runner(executor, chunksize=20000)\n",
    "with workers:\n",
    "    hists = runner(filelist, \"Events\", MyProcessor())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Other resource management modes\n",
    "\n",
    "#### Mixing explicitly declared resources and automatic modes\n",
    "\n",
    "If resources are explicitly declared, such as `WorkQueueExecutor(..., cores=1, memory=4096, disk=8192)`, then Work Queue uses the values given as the maximum allocation a task can use. Otherwise, the maximum allocation is to use a whole worker.\n",
    "\n",
    "#### Maximum throughput mode\n",
    "\n",
    "The default resource management mode is to make new resource allocations using the maximum resources seen. This mode works nicely for most Coffea situations where processing functions use the same amount of cores. However, concurrently may be greatly reduced if there are outliers than use more resources than the rest of the tasks. For such situations, the executor can be directed to use its maximum throughput setting, where Work Queue optimizes the number of tasks done per second, at the expense of retrying outliers using whole workers. For processing tasks that use about the same amount of resources, such as it is often seen in Coffea, both the default and the maximum throughput mode produce the same allocations.\n",
    "\n",
    "```python\n",
    "executor = WorkQueueExecutor(port=port, resources_mode=\"max-throughput\")\n",
    "```\n",
    "\n",
    "Note that the maximum throughput mode, if active, only applies to processing tasks. Accumulating tasks always use the maximum seen allocations because they tend to grow in their resource usage as the run progresses.\n",
    "\n",
    "#### Fixed mode\n",
    "\n",
    "With the fixed mode, the executor does not adapt any of the allocations. If no resource is declared, then tasks use whole workers. Otherwise, undeclared resources are divided proportionally among the resources that were declared.\n",
    "\n",
    "```python\n",
    "executor = WorkQueueExecutor(port=port, resources_mode=\"fixed\", cores=1, memory=4096, disk=8192)\n",
    "```\n",
    "\n",
    "If you want Work Queue to simply manage the resources, but not to enforce them, the resource monitor can be turned off:\n",
    "\n",
    "```python\n",
    "executor = WorkQueueExecutor(port=port, resources_mode=\"fixed\", resource_monitor=\"off\", cores=1, memory=4096, disk=8192)\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When task are to big for the workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tasks whose declared resources don't fit current resources\n",
    "\n",
    "If resources are declared, as in `WorkQueue(..., memory=4000, ...)`, but not connect worker is large enough, then Work Queue **will wait until a larger worker connects.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tasks that during runtime don't fit whole workers\n",
    "\n",
    "Similar to the previous case, when no explicit resources are declared, and the observed resources are larger than the currently connected workers, then Work Queue waits for larger workers to connect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers = Factory(\"local\", manager_host_port = f\"localhost:{port}\")\n",
    "workers.max_workers = 1\n",
    "workers.memory = 4000\n",
    "workers.disk = 10000\n",
    "\n",
    "executor = WorkQueueExecutor(port=port, cores=1, memory=8000)\n",
    "runner = Runner(executor)\n",
    "\n",
    "with workers:\n",
    "    hists = runner(filelist, \"Events\", MyProcessor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When tasks are too big for their explicit allocations\n",
    "\n",
    "For preprocessing and accumulation tasks, if they exhaust explicitly set limits then they fail permanently, and therefore the whole run also fails. This is because these cannot be modified so that they use less resources.\n",
    "\n",
    "Processing tasks are handled differently, and are the topic of the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapting the number of events per processing task (i.e., the chunksize)\n",
    "\n",
    "### Automatic splits\n",
    "\n",
    "When a processing task exhausts its explicitly allocated resources, the set of events is divided and new processing tasks are created. This process is repeated as necessary, until the maximum number of retries is reach (default is 5), or when the chunksize can't be divided anymore.\n",
    "\n",
    "In general, more than a handful of splits is indicative that the desired chunksize is too big, and it should be reduced for further runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To test adapting the chunksize, we use a modified NanoTestProcessor.py,\n",
    "# that artificially increases the memory used as a function of the number\n",
    "# of events in the task.\n",
    "\n",
    "from MemNanoTestProcessor import MemNanoTestProcessor as MyProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also, since this processor is not part of coffea, we need to tell\n",
    "# Work Queue to make it available at the workers. This is convenient,\n",
    "# as we can quickly modify the processor without the need to re-install\n",
    "# or re-build the coffea environments at the remote sites.\n",
    "\n",
    "executor = WorkQueueExecutor(\n",
    "    port=port,\n",
    "    memory=1000,\n",
    "    extra_input_files=[\"MemNanoTestProcessor.py\"])\n",
    "\n",
    "# set a large chunksize, to ensure that task fail\n",
    "runner = Runner(executor, chunksize=256000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers = Factory(\"local\", manager_host_port = f\"localhost:{port}\")\n",
    "workers.max_workers = 1\n",
    "workers.memory = 2000\n",
    "workers.disk = 10000\n",
    "\n",
    "with workers:\n",
    "    hists = runner(filelist, \"Events\", MyProcessor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic chunksize computation\n",
    "\n",
    "The Work Queue executor can also modify the chunksize to a desired memory target usage. In the following example we start with a small chunksize, which grows to fill the desired allocations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_memory = 1000  # 1GB\n",
    "\n",
    "executor = WorkQueueExecutor(\n",
    "    port=port,\n",
    "    extra_input_files=[\"MemNanoTestProcessor.py\"],\n",
    "    memory=target_memory,)                          # do not use more than the target memory\n",
    "                       \n",
    "\n",
    "# set a small chunksize, to create tons of tasks from the single example file.\n",
    "runner = Runner(\n",
    "    executor, \n",
    "    chunksize=1000,\n",
    "    dynamic_chunksize={\"memory\": target_memory})\n",
    "\n",
    "workers = Factory(\"local\", manager_host_port = f\"localhost:{port}\")\n",
    "workers.max_workers = 1\n",
    "workers.memory = 2000\n",
    "workers.disk = 10000\n",
    "\n",
    "with workers:\n",
    "    hists = runner(filelist, \"Events\", MyProcessor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using remote workers\n",
    "\n",
    "### Using a catalog of names\n",
    "\n",
    "All the examples above assumed that Coffea and the workers were running on the same machine, and that we knew which port workers should use to connect to the Work Queue executor. In general, keeping track of addresses and ports is not very practical, but we can use a service to match workers and applications. This service is called the **catalog server**, and by default points to server running at the University of Notre Dame. (You can set catalog servers for yourself.)\n",
    "\n",
    "<img src=\"images/coffea-wq-general-catalog.png\" title=\"Coffea Work Queue basic architecture\" style=\"display:block; margin:auto;\"/>\n",
    "\n",
    "\n",
    "\n",
    "To use the catalog server, we need to give a name to the application, pass this name to the factory, and create a password file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mypassword.txt\", \"w\") as f:\n",
    "    f.write(\"mysecretepassword\")\n",
    "    \n",
    "executor = WorkQueueExecutor(\n",
    "    master_name=\"hal\",\n",
    "    password_file=\"mypassword.txt\",\n",
    "    extra_input_files=[\"MemNanoTestProcessor.py\"])\n",
    "                       \n",
    "runner = Runner(executor, chunksize=65536)\n",
    "\n",
    "workers = Factory(\"local\", \"hal\")\n",
    "workers.password = \"mypassword.txt\"\n",
    "workers.max_workers = 1\n",
    "workers.memory = 2000\n",
    "workers.disk = 10000\n",
    "\n",
    "with workers:\n",
    "    hists = runner(filelist, \"Events\", MyProcessor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an environment\n",
    "\n",
    "In all the examples above we used workers that were running on the same machine as the Coffea application (i.e., we used `Factory(\"local\", ...)`). When a batch system is available, we can direct the factory to launch workers that will run in remote machines, for example, instead of `\"local\"`, the factory accepts `\"condor\"`, `\"slurm\"`, and others.\n",
    "\n",
    "This poses a problem, as it is likely that the remote machines will not have the python environment that is needed for the tasks. The Work Queue executor has the ability to send a python environment together with the tasks, and set it up at run time accordingly. Workers have cache the environment, so that they have to be set only once per Coffea application. \n",
    "\n",
    "To environments that the Work Queue executor expects are files created by the `conda-pack` tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the environment specification\n",
    "import sys\n",
    "\n",
    "# match the python version to be installed with the current one\n",
    "py_ver = f\"{sys.version_info[0]}.{sys.version_info[1]}\"\n",
    "\n",
    "my_env_spec = f\"\"\"\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - coffea\n",
    "  - dill\n",
    "  - xrootd\n",
    "  - python={py_ver}\n",
    "\"\"\"\n",
    "\n",
    "my_env_spec_file = \"my_env_spec.yml\"\n",
    "\n",
    "with open(my_env_spec_file, \"w\") as f:\n",
    "    f.write(my_env_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# shell commands\n",
    "\n",
    "env_name=./myenv_to_send_away_dir\n",
    "env_file=myenv_to_send_away_file.tar.gz\n",
    "\n",
    "\n",
    "# create conda environment in a directory, but only if it is not already there\n",
    "if [[ ! -d \"${env_name}\" ]]\n",
    "then\n",
    "    conda env create -p \"${env_name}\" --file=my_env_spec.yml\n",
    "fi\n",
    "\n",
    "# pack the environment, but only if it is not already there\n",
    "if [[ ! -f \"${env_file}\" ]]\n",
    "then\n",
    "    conda-pack -p \"${env_name}\" -o \"{env_file\"}\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we tell the executor about the environment file we just created:\n",
    "\n",
    "executor = WorkQueueExecutor(\n",
    "    master_name=\"hal\",\n",
    "    environment_file=\"myenv_to_send_away_file.tar.gz\",\n",
    "    password_file=\"mypassword.txt\",\n",
    "    extra_input_files=[\"MemNanoTestProcessor.py\"])\n",
    "                       \n",
    "runner = Runner(executor, chunksize=65536)\n",
    "\n",
    "workers = Factory(\"local\", \"hal\")\n",
    "workers.password = \"mypassword.txt\"\n",
    "workers.max_workers = 1\n",
    "workers.memory = 2000\n",
    "workers.disk = 10000\n",
    "\n",
    "with workers:\n",
    "    hists = runner(filelist, \"Events\", MyProcessor())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The factory outside a notebook\n",
    "\n",
    "Often the workers have to be launched in a machine that is different from the one running the Coffea application. In such cases we cannot use the `with workers:` statement. However, we can still use the factory a shell command. The command takes as an input a configuration file which is monitored for changes. This makes easy to increase the size and number of new workers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factory_conf = \"\"\"\n",
    "\"manager_name\": \"hal\",\n",
    "\"max_workers=2,\n",
    "\"min_workers=0,\n",
    "\"cores\": 4\n",
    "\"memory\": cores * 1024,\n",
    "\"disk\": cores * 2048\n",
    "\"\"\"\n",
    "\n",
    "with open(\"factory.jx\", \"w\") as f:\n",
    "    f.write(factory_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the shell, for example:\n",
    "# work_queue_factory -T condor -C factory.jx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
