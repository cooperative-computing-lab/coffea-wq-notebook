{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HEP analysis using Coffea + Work Queue Executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatically reload python modules when they are modified\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Work Queue](https://cctools.readthedocs.io/en/stable/work_queue/) is a system and library for creating and managing scalable manager-worker style programs that scale up to thousands of machines on clusters, clouds, and grids. With Work Queue, Coffea sends the work to be done to a set of Work Queue workers. These workers may be running locally, remotely in a cluster, or in some container environment.\n",
    "\n",
    "<img src=\"images/coffea-wq-general.png\" title=\"Coffea Work Queue basic architecture\" style=\"display:block; margin:auto;\"/>\n",
    "\n",
    "When used together with Coffea, it can measure the resources, such as cores and memory, that chunks of events need and adapt the allocations automatically to maximize throughput. In this notebook we will demonstrate how the executor can dynamically modify the size of chunks of events to process if the memory available is not enough, and adapt it to desired resources usage. But as a starter, let's first introduce its basic use for small local runs.\n",
    "\n",
    "and how it can automatically\n",
    "export the needed python environments when working in a cluster with no\n",
    "previous setup.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load coffea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# all coffea applications are run by coffea.processor.Runner\n",
    "from coffea.processor import Runner\n",
    "from coffea.processor import WorkQueueExecutor\n",
    "\n",
    "# workers will be created using this Factory\n",
    "from work_queue import Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test processor, modified from coffea source to make memory usage more interesting\n",
    "#from MemNanoTestProcessor import MemNanoTestProcessor as MyProcessor\n",
    "\n",
    "# or you can use the test processor included in coffea:\n",
    "\n",
    "# Use a sample processor from coffea\n",
    "# and silence coffea.hist deprecation, for now\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
    "from coffea.processor.test_items import NanoTestProcessor as MyProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define example input file. \n",
    "\n",
    "filelist = {\n",
    "    \"ttHJet\": [\"http://www.crc.nd.edu/~kmohrman/files/root_files/for_ci/ttHJet_UL17_R1B14_NAOD-00000_10194.root\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic configure for the WorkQueue Executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# For the most basic Work Queue configuration, we need to set the port that\n",
    "# the workers will use to communicate with the Coffea application.\n",
    "# The default for Work Queue is 9123, so we will use that here.\n",
    "# We can set Work Queue to find a free port for use, and we will explore\n",
    "# that later in the notebook.\n",
    "port = 9123\n",
    "executor = WorkQueueExecutor(port=port)\n",
    "runner = Runner(executor)\n",
    "\n",
    "#executor = WorkQueueExecutor(port=port, status_display_interval=5, extra_input_files=[\"MemNanoTestProcessor.py\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running one function at a time\n",
    "\n",
    "In Coffea, the iterative executor executes locally one function at a time. We can emulate this by directing the factory that creates workers to only create one worker. The factory automatically creates workers as it sees work available up to `max_workers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a factory that looks for a running Coffea in the local machine at the\n",
    "# port we defined above.\n",
    "workers = Factory(\"local\", manager_host_port = f\"localhost:{port}\")\n",
    "\n",
    "# Tell the factory that it should not create more than one worker...\n",
    "workers.max_workers = 1\n",
    "\n",
    "# ...and that the worker should not use more than 10GB of memory. If we do not set\n",
    "# resource limits, the worker automatically configures to use all available\n",
    "# in the machine, so the worker will use, for example, all the cores and memory in the\n",
    "# local machine.\n",
    "workers.disk = 10000  # Each worker won't use more than 10GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f09813b2e3994ec3889c460a71ec7a4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridspecLayout(children=(VBox(children=(HTML(value=''), HTML(value='')), layout=Layout(grid_area='widget001'))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening for work queue workers on port 9123.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09c62cab42f140638772220765aec3d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing   0%|          | 0/1 [00:00<?, ?file/s]                               "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7894292c87fd49448eee2801f595c080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Submitted       0%|          | 0/232632 [00:00<?, ?event/s]                         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f89d061f96ec408a89c9a0e241342784",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing      0%|          | 0/232632 [00:00<?, ?event/s]                         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2aab892a7564fb99518a4f5264650c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accumulated     0%|          | 0/1 [00:00<?, ?task/s]                               "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Finally, we execute the Coffea application. By using the factory inside a \n",
    "# \"with\" statement, the factory shutdowns the workers when the workflow finishes.\n",
    "\n",
    "with workers:\n",
    "    hists = runner(filelist, \"Events\", MyProcessor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we can plot the results as usual\n",
    "%matplotlib inline\n",
    "import coffea.hist\n",
    "from IPython.utils import io\n",
    "coffea.hist.plot1d(hists[\"mass\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually declaring resources\n",
    "\n",
    "As we saw above, each task (that is, each function) used at most 1 core, and only a small portion of the memory and disk available. Thus, if our worker has more than 1 core, it could run more than one task concurrently. This can be achieved by explicitely specifying how many resources a task should use. Note how Work Queue automatically allocates to the tasks a corresponding proportion for the resources that we did not specify, (i.e. memory and disk):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a56e0fe21f97444c99a40175cbbc287d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridspecLayout(children=(VBox(children=(HTML(value=''), HTML(value='')), layout=Layout(grid_area='widget001'))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening for work queue workers on port 9123.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6b0f8912aa3417a9f96e4b93df3ac40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Submitted       0%|          | 0/232632 [00:00<?, ?event/s]                         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28ef084dc4a94f7e851858dfb4121c06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing      0%|          | 0/232632 [00:00<?, ?event/s]                         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "252b0290ffe24e6f96a77db71268f947",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accumulated     0%|          | 0/1 [00:00<?, ?task/s]                               "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "executor = WorkQueueExecutor(port=port, cores=1)\n",
    "runner = Runner(executor)\n",
    "with workers:\n",
    "    hists = runner(filelist, \"Events\", MyProcessor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic resource management\n",
    "\n",
    "The previous example worked because we knew that each task did not use more than one core. Since we do not always know how many resouces a function will need, the Work Queue executor automatically adjust its allocations according to previous values measured. Preprocessing, processing, and accumulating tasks are handled separately\n",
    "\n",
    "In fact, this is the default mode for the executor, only that we did not observe this in the previous examples because given the default coffea chunkise (100k events), there were not enough tasks for Work Queue to figure out an allocation. In the absence of data, Work Queue allocates a whole worker to a task. Thus, for the automatic resource management example, we set the chunksize to a small value to create tons of tasks. Later in this notebook we will show how Work Queue can also adap the chunksize to better use the resource allocations. \n",
    "\n",
    "In the output below, note how the `current allocation` of processing tasks eventually adapts to 1 core, even though there is no mention of cores when declaring the executor.\n",
    "\n",
    "Not that the automatic resource management depends on measuring the resources used by coffea tasks, and this only works on Linux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a12befd62a284436b958ad8f0c6b5e97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridspecLayout(children=(VBox(children=(HTML(value=''), HTML(value='')), layout=Layout(grid_area='widget001'))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening for work queue workers on port 9123.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c6fbb9f82744434af9ffa5dd992f03d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Submitted       0%|          | 0/232632 [00:00<?, ?event/s]                         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5498d3e7bdd4cdd9e8c723cc0c14816",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing      0%|          | 0/232632 [00:00<?, ?event/s]                         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5889c54083ea4b348d897212ae62a6dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accumulated     0%|          | 0/1 [00:00<?, ?task/s]                               "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "executor = WorkQueueExecutor(port=port)\n",
    "runner = Runner(executor, chunksize=20000)\n",
    "with workers:\n",
    "    hists = runner(filelist, \"Events\", MyProcessor())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Other resource management modes\n",
    "\n",
    "#### Mixing explicitely declared resources and automatic modes\n",
    "\n",
    "If resources are explicitely declared, such as `WorkQueueExecutor(..., cores=1, memory=4096, disk=8192)`, then Work Queue uses the values given as the maximum allocation a task can use. Otherwise, the maximum allocation is to use a whole worker.\n",
    "\n",
    "#### Maximum throughput mode\n",
    "\n",
    "The default resource management mode is to make new resource allocations using the maximum resources seen. This mode works nicely for most Coffea situations where processing functions use the same amount of cores. However, concurrenty may be greatly reduced if there are outliers than use more resources than the rest of the tasks. For such situations, the executor can be directed to use its maximum throughput setting, where Work Queue optimizes the number of tasks done per second, at the expense of retrying outliers using whole workers. For processing tasks that use about the same amount of resources, such as it is often seen in Coffea, both the default and the maximum throughput mode produce the same allocations.\n",
    "\n",
    "```python\n",
    "executor = WorkQueueExecutor(port=port, resources_mode=\"max-throughput\")\n",
    "```\n",
    "\n",
    "Note that the maximum throughput mode, if active, only applies to processing tasks. Accumulating tasks always use the maximum seen allocations because they tend to grow in their resource usage as the run progresses.\n",
    "\n",
    "#### Fixed mode\n",
    "\n",
    "With the fixed mode, the executor does not adapt any of the allocations. If no resource is declared, then tasks use whole workers. Otherwise, undeclared resources are divided proportionally among the resources that were declared.\n",
    "\n",
    "```python\n",
    "executor = WorkQueueExecutor(port=port, resources_mode=\"fixed\", cores=1, memory=4096, disk=8192)\n",
    "```\n",
    "\n",
    "If you want Work Queue to simply manage the resources, but not to enforce them, the resource monitor can be turned off:\n",
    "\n",
    "```python\n",
    "executor = WorkQueueExecutor(port=port, resources_mode=\"fixed\", resource_monitor=\"off\", cores=1, memory=4096, disk=8192)\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When task don't fit workers\n",
    "\n",
    "#### Explicitely declared resources\n",
    "\n",
    "If task resources are explicitely declared, but no connected worker is large enough to fit the task, then Work Queue will wait until a larger worker connects.\n",
    "\n",
    "(In a notebook is hard to feed new workers, as execution blocks until a cell is completed. At the end of the notebook we present how to launch workers outside a notebook.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing and accumulating tasks that don't fit workers\n",
    "\n",
    "If no explicit resources are declared, and the observed resources of preprocessing and accumulating tasks are larger than the currently connected workers, then Work Queu waits for larger workers to connect. Otherwise, if explicit resources were declared, the workflow fails.\n",
    "\n",
    "Processing tasks are handled differently, as we present below dynamic chunksize strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7222feb3c3eb4eab8e2d98ce0b86aab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridspecLayout(children=(VBox(children=(HTML(value=''), HTML(value='')), layout=Layout(grid_area='widget001'))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening for work queue workers on port 9123.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03cb9652c4e34213a92287fd38edb398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Submitted       0%|          | 0/232632 [00:00<?, ?event/s]                         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "088ba1421e9249e0adb1ce8fcdaddd44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing      0%|          | 0/232632 [00:00<?, ?event/s]                         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15e2ae391d79451183fa015e0ed81f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accumulated     0%|          | 0/1 [00:00<?, ?task/s]                               "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "workers = Factory(\"local\", manager_host_port = f\"localhost:{port}\")\n",
    "workers.max_workers = 1\n",
    "workers.memory = 4000\n",
    "workers.disk = 10000\n",
    "\n",
    "executor = WorkQueueExecutor(port=port, cores=1, memory=8000)\n",
    "runner = Runner(executor)\n",
    "with workers:\n",
    "    hists = runner(filelist, \"Events\", MyProcessor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapting the number of events per processing task (i.e., the chunksize)\n",
    "\n",
    "### Automatic splits\n",
    "\n",
    "When a processing task exhausts its explicitely allocated resources, the set of events is divided and new processing tasks are created. This process is repeated as necessary, until the maximum number of retries is reach (default is 5), or when the chunksize can't be divided anymore.\n",
    "\n",
    "In general, more than a handful of splits is indicative that the desired chunksize is too big, and it should be reduced for further runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To test adapting the chunksize, we use a modified NanoTestProcessor.py,\n",
    "# that artificially increases the memory used as a function of the number\n",
    "# of events in the task.\n",
    "\n",
    "from MemNanoTestProcessor import MemNanoTestProcessor as MyProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also, since this processor is not part of coffea, we need to tell\n",
    "# Work Queue to make it available at the workers. This is convenient,\n",
    "# as we can quickly modify the processor without the need to re-install\n",
    "# or re-build the coffea environments at the remote sites.\n",
    "\n",
    "executor = WorkQueueExecutor(\n",
    "    port=port,\n",
    "    memory=1000,\n",
    "    extra_input_files=[\"MemNanoTestProcessor.py\"])\n",
    "\n",
    "# set a large chunksize, to ensure that task fail\n",
    "runner = Runner(executor, chunksize=256000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30d039982a5c4d259e04f34c0027deea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridspecLayout(children=(VBox(children=(HTML(value=''), HTML(value='')), layout=Layout(grid_area='widget001'))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening for work queue workers on port 9123.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ef104622abd433a835fb8f503421e41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Submitted       0%|          | 0/232632 [00:00<?, ?event/s]                         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "979286a1a9944da9b003dee05bf62c8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing      0%|          | 0/232632 [00:00<?, ?event/s]                         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d296ed4706442ecbe4586509cce14f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accumulated     0%|          | 0/1 [00:00<?, ?task/s]                               "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing task id 1 item p_31 with 232632 events completed on cclws16.cse.nd.edu. return code 143\n",
      "task id 1 item p_31 failed: RESOURCE_EXHAUSTION\n",
      "    ('ttHJet', 'http://www.crc.nd.edu/~kmohrman/files/root_files/for_ci/ttHJet_UL17_R1B14_NAOD-00000_10194.root', 'Events', 0, 232632) p_31 without result.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing task id 2 item p_32 with 116316 events completed on cclws16.cse.nd.edu. return code 143\n",
      "task id 2 item p_32 failed: RESOURCE_EXHAUSTION\n",
      "    ('ttHJet', 'http://www.crc.nd.edu/~kmohrman/files/root_files/for_ci/ttHJet_UL17_R1B14_NAOD-00000_10194.root', 'Events', 0, 116316) p_32 without result.\n",
      "processing task id 3 item p_33 with 116316 events completed on cclws16.cse.nd.edu. return code 143\n",
      "task id 3 item p_33 failed: RESOURCE_EXHAUSTION\n",
      "    ('ttHJet', 'http://www.crc.nd.edu/~kmohrman/files/root_files/for_ci/ttHJet_UL17_R1B14_NAOD-00000_10194.root', 'Events', 116316, 232632) p_33 without result.\n"
     ]
    }
   ],
   "source": [
    "workers = Factory(\"local\", manager_host_port = f\"localhost:{port}\")\n",
    "workers.max_workers = 1\n",
    "workers.memory = 2000\n",
    "workers.disk = 10000\n",
    "\n",
    "with workers:\n",
    "    hists = runner(filelist, \"Events\", MyProcessor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic chunksize computation\n",
    "\n",
    "The Work Queue executor can also modify the chunksize to a desired memory target usage. In the following example we start with a small chunksize, which grows to fill the desired allocations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c52beb0c8e3468fadae01c74c8b3ecf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridspecLayout(children=(VBox(children=(HTML(value=''), HTML(value='')), layout=Layout(grid_area='widget001'))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening for work queue workers on port 9123.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffa589546c5242649b49e5aa6131b23c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Submitted       0%|          | 0/232632 [00:00<?, ?event/s]                         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4126cb89e3204c3aa1235482931a18b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing      0%|          | 0/232632 [00:00<?, ?event/s]                         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96d29efb9ba14a0f96b69fa0e0a15838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accumulated     0%|          | 0/10 [00:00<?, ?task/s]                              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "target_memory = 1000  # 1GB\n",
    "\n",
    "executor = WorkQueueExecutor(\n",
    "    port=port,\n",
    "    extra_input_files=[\"MemNanoTestProcessor.py\"],\n",
    "    memory=target_memory)                        # do not use more than the target memory\n",
    "\n",
    "# set a small chunksize, to create tons of tasks from the single example file.\n",
    "runner = Runner(\n",
    "    executor, \n",
    "    chunksize=1000,\n",
    "    dynamic_chunksize={\"memory\": target_memory})\n",
    "\n",
    "workers = Factory(\"local\", manager_host_port = f\"localhost:{port}\")\n",
    "workers.max_workers = 1\n",
    "workers.memory = 2000\n",
    "workers.disk = 10000\n",
    "\n",
    "with workers:\n",
    "    hists = runner(filelist, \"Events\", MyProcessor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using remote workers\n",
    "\n",
    "### Catalog server\n",
    "\n",
    "### Create an environment\n",
    "\n",
    "### The factory outside a notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
